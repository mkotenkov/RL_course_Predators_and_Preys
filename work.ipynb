{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/maksimkoltugin/Desktop/код/ВШЭ/RL_course_Predators_and_Preys/.venv/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from modules.utils import TrainConfig, Logger, paint, get_env, simulate_episode\n",
    "from modules.DQN import DQN\n",
    "from modules.reward import Reward\n",
    "from modules.preprocess import preprocess\n",
    "\n",
    "import numpy as np\n",
    "import pickle\n",
    "from tqdm.auto import trange, tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "\n",
    "import random\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import os\n",
    "\n",
    "# 1 right\n",
    "# 2 left\n",
    "# 3 up\n",
    "# 4 down\n",
    "\n",
    "# general settings\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "N_ACTIONS = 5\n",
    "N_PREDATORS = 5 # change to 5 !!!!!\n",
    "N_MASKS = 5 # after preprocessing\n",
    "MAP_SIZE = 40\n",
    "\n",
    "# train settings\n",
    "cfg = TrainConfig(\n",
    "    description='some description',    \n",
    "    max_steps_for_episode=300, # change to 300 !!!!!\n",
    "    gamma = 0.9, # maybe better to set less !!!!!\n",
    "    initial_steps=1000, # change to 100000 !!!!!\n",
    "    steps=100_000,\n",
    "    steps_per_update=3,\n",
    "    steps_per_paint=250,\n",
    "    steps_per_gif=500,\n",
    "    buffer_size=10_000,\n",
    "    batch_size=64,\n",
    "    learning_rate=1e-3,\n",
    "    eps_start=0.9, \n",
    "    eps_end=0.05,\n",
    "    eps_decay=1000,\n",
    "    tau=0.01, # the update rate of the target network, was 0.005\n",
    "    reward_weights=dict(\n",
    "        w_kill_prey=1.,\n",
    "        w_kill_enemy=2.3,\n",
    "        w_kill_bonus=1.3, \n",
    "        gamma_for_bonus_count=0.5,\n",
    "        w_dummy_step=-0.23\n",
    "    ),\n",
    "    seed=1234 \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TRAIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "00.4974958263772955\n"
     ]
    }
   ],
   "source": [
    "from modules.reward import get_state_value\n",
    "\n",
    "def softmax(x):\n",
    "    \"\"\"Compute softmax values for each sets of scores in x.\"\"\"\n",
    "    e_x = np.exp(x - np.max(x))\n",
    "    return e_x / e_x.sum()\n",
    "\n",
    "\n",
    "class RewardBasedModel:\n",
    "    def __init__(self, reward_weights):\n",
    "        self.reward_weights = reward_weights\n",
    "        self.rewards = dict()\n",
    "\n",
    "    def get_actions(self, processed_state, info):\n",
    "        # initial_sv = get_state_value(cfg.reward_weights, processed_state)\n",
    "        rewards = []\n",
    "        for name, x, y in [(\"right\", 21, 20), (\"left\", 19, 20), (\"up\", 20, 19), (\"down\", 20, 21)]:\n",
    "            sv = get_state_value(cfg.reward_weights, processed_state, info, (x, y))\n",
    "            reward = sv + self.__get_kills_value(x, y, processed_state)\n",
    "            reward = reward * self.__is_cell_empty(x, y, processed_state)\n",
    "            rewards.append(reward)\n",
    "            self.rewards[name] = reward\n",
    "        rewards = np.stack(rewards)\n",
    "        return np.argmax(rewards, axis=0) + 1\n",
    "\n",
    "    def __get_weight_from_coordinates(self, x, y, processed_state):\n",
    "        _, preys_mask, enemies_mask, bonuses_mask, _ = processed_state\n",
    "\n",
    "        if preys_mask[y, x] == 1:\n",
    "            return self.reward_weights[\"w_kill_prey\"]\n",
    "        if enemies_mask[y, x] == 1:\n",
    "            return self.reward_weights[\"w_kill_enemy\"]\n",
    "        if bonuses_mask[y, x] == 1:\n",
    "            return self.reward_weights[\"w_kill_bonus\"]\n",
    "\n",
    "        return 0\n",
    "\n",
    "    def __get_kills_value(self, x, y, processed_state):\n",
    "        out = []\n",
    "        for pr_st in processed_state:\n",
    "            out.append(self.__get_weight_from_coordinates(x, y, pr_st))\n",
    "        return np.stack(out)\n",
    "\n",
    "    def __is_cell_empty(self, x, y, processed_state):\n",
    "        out = []\n",
    "        for pr_st in processed_state:\n",
    "            stones_mask, *_ = pr_st\n",
    "            out.append(stones_mask[y, x] == 0)\n",
    "        return np.array(out)\n",
    "\n",
    "\n",
    "def evaluate(model, n_episodes=3):\n",
    "    results = []\n",
    "    for step, d in enumerate(np.linspace(0, 1, n_episodes)):  \n",
    "        print('\\r' + str(step), end='')      \n",
    "        results.append(simulate_episode(model, d, N_PREDATORS, cfg, '00000.gif', render_gif=False))\n",
    "    return sum(results) / len(results)\n",
    "\n",
    "# ys = []\n",
    "# xs = np.linspace(-0.15, -0.35, 8)\n",
    "# for i, w_dummy_step in tqdm(enumerate(xs)):\n",
    "#     weights = cfg.reward_weights.copy()\n",
    "#     weights[\"w_dummy_step\"] = w_dummy_step\n",
    "#     model = RewardBasedModel(weights)\n",
    "#     ys.append(evaluate(model))\n",
    "print(evaluate(RewardBasedModel(cfg.reward_weights), 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'xs' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/Users/maksimkoltugin/Desktop/код/ВШЭ/RL_course_Predators_and_Preys/work.ipynb Cell 4\u001b[0m line \u001b[0;36m3\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/maksimkoltugin/Desktop/%D0%BA%D0%BE%D0%B4/%D0%92%D0%A8%D0%AD/RL_course_Predators_and_Preys/work.ipynb#X21sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mmatplotlib\u001b[39;00m \u001b[39mimport\u001b[39;00m pyplot \u001b[39mas\u001b[39;00m plt\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/maksimkoltugin/Desktop/%D0%BA%D0%BE%D0%B4/%D0%92%D0%A8%D0%AD/RL_course_Predators_and_Preys/work.ipynb#X21sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m plt\u001b[39m.\u001b[39mplot(xs, ys)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/maksimkoltugin/Desktop/%D0%BA%D0%BE%D0%B4/%D0%92%D0%A8%D0%AD/RL_course_Predators_and_Preys/work.ipynb#X21sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m plt\u001b[39m.\u001b[39mxlabel(\u001b[39m'\u001b[39m\u001b[39mw_dummy_step\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/maksimkoltugin/Desktop/%D0%BA%D0%BE%D0%B4/%D0%92%D0%A8%D0%AD/RL_course_Predators_and_Preys/work.ipynb#X21sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m plt\u001b[39m.\u001b[39mylabel(\u001b[39m'\u001b[39m\u001b[39mscore / (bot_score + score)\u001b[39m\u001b[39m'\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'xs' is not defined"
     ]
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "plt.plot(xs, ys)\n",
    "plt.xlabel('w_dummy_step')\n",
    "plt.ylabel('score / (bot_score + score)')\n",
    "plt.title('score / (bot_score + score)')\n",
    "plt.axis(ymin=0, ymax=1)\n",
    "plt.axhline(0.5, color='red', linestyle='--')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "__init__() got an unexpected keyword argument 'w_kill_prey'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/Users/maksimkoltugin/Desktop/код/ВШЭ/RL_course_Predators_and_Preys/work.ipynb Cell 5\u001b[0m line \u001b[0;36m9\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/maksimkoltugin/Desktop/%D0%BA%D0%BE%D0%B4/%D0%92%D0%A8%D0%AD/RL_course_Predators_and_Preys/work.ipynb#W3sZmlsZQ%3D%3D?line=90'>91</a>\u001b[0m         model\u001b[39m.\u001b[39msave(logger\u001b[39m.\u001b[39mcurr_subfolder_path \u001b[39m+\u001b[39m \u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39m/model_steps_\u001b[39m\u001b[39m{\u001b[39;00mmodel\u001b[39m.\u001b[39msteps_done\u001b[39m}\u001b[39;00m\u001b[39m.pt\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/maksimkoltugin/Desktop/%D0%BA%D0%BE%D0%B4/%D0%92%D0%A8%D0%AD/RL_course_Predators_and_Preys/work.ipynb#W3sZmlsZQ%3D%3D?line=91'>92</a>\u001b[0m         \u001b[39mreturn\u001b[39;00m model\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/maksimkoltugin/Desktop/%D0%BA%D0%BE%D0%B4/%D0%92%D0%A8%D0%AD/RL_course_Predators_and_Preys/work.ipynb#W3sZmlsZQ%3D%3D?line=94'>95</a>\u001b[0m model \u001b[39m=\u001b[39m train()\n",
      "\u001b[1;32m/Users/maksimkoltugin/Desktop/код/ВШЭ/RL_course_Predators_and_Preys/work.ipynb Cell 5\u001b[0m line \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/maksimkoltugin/Desktop/%D0%BA%D0%BE%D0%B4/%D0%92%D0%A8%D0%AD/RL_course_Predators_and_Preys/work.ipynb#W3sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m state, info \u001b[39m=\u001b[39m env\u001b[39m.\u001b[39mreset()    \n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/maksimkoltugin/Desktop/%D0%BA%D0%BE%D0%B4/%D0%92%D0%A8%D0%AD/RL_course_Predators_and_Preys/work.ipynb#W3sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m processed_state \u001b[39m=\u001b[39m preprocess(state, info)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/maksimkoltugin/Desktop/%D0%BA%D0%BE%D0%B4/%D0%92%D0%A8%D0%AD/RL_course_Predators_and_Preys/work.ipynb#W3sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m r \u001b[39m=\u001b[39m Reward(n_predators\u001b[39m=\u001b[39;49mN_PREDATORS, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mcfg\u001b[39m.\u001b[39;49mreward_weights)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/maksimkoltugin/Desktop/%D0%BA%D0%BE%D0%B4/%D0%92%D0%A8%D0%AD/RL_course_Predators_and_Preys/work.ipynb#W3sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m \u001b[39mfor\u001b[39;00m _ \u001b[39min\u001b[39;00m trange(cfg\u001b[39m.\u001b[39minitial_steps):\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/maksimkoltugin/Desktop/%D0%BA%D0%BE%D0%B4/%D0%92%D0%A8%D0%AD/RL_course_Predators_and_Preys/work.ipynb#W3sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m     actions \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mget_actions(processed_state, random\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n",
      "\u001b[0;31mTypeError\u001b[0m: __init__() got an unexpected keyword argument 'w_kill_prey'"
     ]
    }
   ],
   "source": [
    "def train():\n",
    "    model = DQN(\n",
    "        n_masks=N_MASKS,\n",
    "        n_actions=N_ACTIONS,\n",
    "        n_predators=N_PREDATORS,\n",
    "        map_size=MAP_SIZE,\n",
    "        device=DEVICE,\n",
    "        config=cfg\n",
    "    ).to(DEVICE).train()\n",
    "\n",
    "    logger = Logger(cfg)\n",
    "\n",
    "    # INITIAL STEPS\n",
    "    env = get_env(n_predators=N_PREDATORS, difficulty=0, step_limit=cfg.max_steps_for_episode)\n",
    "    state, info = env.reset()    \n",
    "    processed_state = preprocess(state, info)\n",
    "    r = Reward(n_predators=N_PREDATORS, **cfg.reward_weights)\n",
    "    for _ in trange(cfg.initial_steps):\n",
    "        actions = model.get_actions(processed_state, random=True)\n",
    "        next_state, done, next_info = env.step(actions)\n",
    "        next_processed_state = preprocess(next_state, next_info)\n",
    "        reward = r(processed_state, info, next_processed_state, next_info)\n",
    "        model.consume_transition(processed_state, actions, next_processed_state, reward, done)\n",
    "        state, info = (next_state, next_info) if not done else env.reset()        \n",
    "        processed_state = preprocess(state, info)\n",
    "        \n",
    "\n",
    "    # with open(f'pre_calc_buffer_simple_10000.pkl', 'wb') as handle:\n",
    "    #     pickle.dump(model.buffer, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "    # TRAINING\n",
    "    # with open('pre_calc_buffer_simple_10000.pkl', 'rb') as handle:\n",
    "    #     model.buffer = pickle.load(handle)\n",
    "\n",
    "    env = get_env(n_predators=N_PREDATORS, difficulty=0, step_limit=cfg.max_steps_for_episode)\n",
    "    state, info = env.reset()\n",
    "    processed_state = preprocess(state, info)\n",
    "    r = Reward(n_predators=N_PREDATORS, **cfg.reward_weights)\n",
    "    score_difference = None\n",
    "\n",
    "    try:\n",
    "        for _ in trange(cfg.steps):\n",
    "            # ========== step ==========================================================\n",
    "            eps_threshold = cfg.eps_end + (cfg.eps_start - cfg.eps_end) * \\\n",
    "                math.exp(-1. * model.steps_done / cfg.eps_decay)\n",
    "            actions = model.get_actions(processed_state, random=(random.random() < eps_threshold))\n",
    "            next_state, done, next_info = env.step(actions)\n",
    "            next_processed_state = preprocess(next_state, next_info)\n",
    "            reward = r(processed_state, info, next_processed_state, next_info)\n",
    "            model.consume_transition(processed_state, actions, next_processed_state, reward, done)\n",
    "\n",
    "            if done:\n",
    "                # not just reset in oreder to implement changes of map\n",
    "                env = get_env(n_predators=N_PREDATORS, difficulty=model.steps_done/cfg.steps,\n",
    "                              step_limit=cfg.max_steps_for_episode)\n",
    "                state, info = env.reset()\n",
    "            else:\n",
    "                state, info = next_state, next_info\n",
    "\n",
    "            processed_state = preprocess(state, info)\n",
    "\n",
    "            # ========== updates =======================================================\n",
    "            if model.steps_done % cfg.steps_per_update == 0:\n",
    "                reward_batch, loss = model.update_policy_network()\n",
    "\n",
    "            model.soft_update_target_network()  # each step\n",
    "\n",
    "            if model.steps_done % cfg.steps_per_paint == 0 and model.steps_done > 0:\n",
    "                paint(logger, [['reward', 'reward_batch', 'loss'], ['score_difference']])\n",
    "\n",
    "            if model.steps_done % cfg.steps_per_gif == 0 and model.steps_done > 0:\n",
    "                os.makedirs(logger.curr_subfolder_path + '/gifs', exist_ok=True)\n",
    "                path = f'{logger.curr_subfolder_path}/gifs/{model.steps_done}_steps.gif'\n",
    "                score_difference = simulate_episode_and_create_gif(\n",
    "                    model, model.steps_done/cfg.steps, N_PREDATORS, cfg, path)\n",
    "\n",
    "            model.steps_done += 1\n",
    "\n",
    "            # ========== logs ==========================================================\n",
    "            logger.add('eps', eps_threshold)\n",
    "            logger.add('reward', reward.mean())\n",
    "            logger.add('reward_batch', reward_batch)\n",
    "            logger.add('loss', loss)\n",
    "            logger.add('score_difference', score_difference)\n",
    "\n",
    "    except KeyboardInterrupt:\n",
    "        print('Training interrupted')\n",
    "\n",
    "    finally:\n",
    "        logger.save()\n",
    "        model.save(logger.curr_subfolder_path + f'/model_steps_{model.steps_done}.pt')\n",
    "        return model\n",
    "\n",
    "\n",
    "model = train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO\n",
    "\n",
    "0. разобраться с гитом\n",
    "0. ивалуейт. сделать 5 итераций, степс_пер_криейт_гиф увеличить в 5 раз\n",
    "0. сделать реворд-2\n",
    "0. зафорсить оптимальные действия в инишал буффер ??\n",
    "0. добавить шедулер ??\n",
    "2. добавить разделение на группы в пейнтр\n",
    "4. добавить MA to paint\n",
    "6. если заработает бейзлайн, подумать как добавить возм-ть выучить \"бфс\"\n",
    "7. способ ускорить реворд - сохранять последний СВ\n",
    "8. изменить отрисовку на фиолетовую\n",
    "9. добавить учёт разницы в счете ?.."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# список костылей\n",
    "\n",
    "2. плохо проработан момент съедения жертвы\n",
    "4. костыль RewardBasedModel: если идёт в стену, то может получить положительный реворд"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# заметки\n",
    "\n",
    "1. слишком высокий вес енеми заставляет за ним гоняться и это выходит тупо. \n",
    "2. в случае если кого-то кушаю, реворд должен учитывать дельтуСВ, но при этом она НЕ должна быть меньше 0\n",
    "3. отслеживать респавн через инфо, учитывать в реворде\n",
    "4. сделать чтобы после n-го шага считался более дешевый в вычислении реворд\n",
    "5. сделать удорожание каждого последующего dummy_step'a\n",
    "6. (возможно) сделать рассеивание местоположения далёких жертв (что это даст?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'pre_calculated_buffer_10000.pkl'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/maksimkoltugin/Desktop/код/ВШЭ/RL_course_Predators_and_Preys/work.ipynb Cell 8\u001b[0m line \u001b[0;36m1\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/maksimkoltugin/Desktop/%D0%BA%D0%BE%D0%B4/%D0%92%D0%A8%D0%AD/RL_course_Predators_and_Preys/work.ipynb#W6sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m model \u001b[39m=\u001b[39m DQN(\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/maksimkoltugin/Desktop/%D0%BA%D0%BE%D0%B4/%D0%92%D0%A8%D0%AD/RL_course_Predators_and_Preys/work.ipynb#W6sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m     n_masks\u001b[39m=\u001b[39mN_MASKS,\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/maksimkoltugin/Desktop/%D0%BA%D0%BE%D0%B4/%D0%92%D0%A8%D0%AD/RL_course_Predators_and_Preys/work.ipynb#W6sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m     n_actions\u001b[39m=\u001b[39mN_ACTIONS,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/maksimkoltugin/Desktop/%D0%BA%D0%BE%D0%B4/%D0%92%D0%A8%D0%AD/RL_course_Predators_and_Preys/work.ipynb#W6sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m     config\u001b[39m=\u001b[39mcfg\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/maksimkoltugin/Desktop/%D0%BA%D0%BE%D0%B4/%D0%92%D0%A8%D0%AD/RL_course_Predators_and_Preys/work.ipynb#W6sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m )\u001b[39m.\u001b[39mto(DEVICE)\u001b[39m.\u001b[39mtrain()\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/maksimkoltugin/Desktop/%D0%BA%D0%BE%D0%B4/%D0%92%D0%A8%D0%AD/RL_course_Predators_and_Preys/work.ipynb#W6sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39;49m(\u001b[39m'\u001b[39;49m\u001b[39mpre_calculated_buffer_10000.pkl\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39m'\u001b[39;49m\u001b[39mrb\u001b[39;49m\u001b[39m'\u001b[39;49m) \u001b[39mas\u001b[39;00m handle:\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/maksimkoltugin/Desktop/%D0%BA%D0%BE%D0%B4/%D0%92%D0%A8%D0%AD/RL_course_Predators_and_Preys/work.ipynb#W6sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m     buffer \u001b[39m=\u001b[39m pickle\u001b[39m.\u001b[39mload(handle)\n",
      "File \u001b[0;32m~/Desktop/код/ВШЭ/RL_course_Predators_and_Preys/.venv/lib/python3.9/site-packages/IPython/core/interactiveshell.py:286\u001b[0m, in \u001b[0;36m_modified_open\u001b[0;34m(file, *args, **kwargs)\u001b[0m\n\u001b[1;32m    279\u001b[0m \u001b[39mif\u001b[39;00m file \u001b[39min\u001b[39;00m {\u001b[39m0\u001b[39m, \u001b[39m1\u001b[39m, \u001b[39m2\u001b[39m}:\n\u001b[1;32m    280\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    281\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mIPython won\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt let you open fd=\u001b[39m\u001b[39m{\u001b[39;00mfile\u001b[39m}\u001b[39;00m\u001b[39m by default \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    282\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    283\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39myou can use builtins\u001b[39m\u001b[39m'\u001b[39m\u001b[39m open.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    284\u001b[0m     )\n\u001b[0;32m--> 286\u001b[0m \u001b[39mreturn\u001b[39;00m io_open(file, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'pre_calculated_buffer_10000.pkl'"
     ]
    }
   ],
   "source": [
    "model = DQN(\n",
    "    n_masks=N_MASKS,\n",
    "    n_actions=N_ACTIONS,\n",
    "    n_predators=N_PREDATORS,\n",
    "    map_size=MAP_SIZE,\n",
    "    device=DEVICE,\n",
    "    config=cfg\n",
    ").to(DEVICE).train()\n",
    "\n",
    "with open('pre_calculated_buffer_10000.pkl', 'rb') as handle:\n",
    "    buffer = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "        #     rews.append(reward)\n",
    "        # rews = np.stack(rews)\n",
    "        # sft = softmax(rews)\n",
    "        # actions = []\n",
    "        # for i in range(5):\n",
    "        #     action = np.random.choice(np.arange(4), p=sft[:, i]) + 1\n",
    "        #     actions.append(action)\n",
    "        # return actions"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
